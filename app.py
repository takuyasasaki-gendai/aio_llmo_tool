import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin, quote
import json
import re
import urllib3
from duckduckgo_search import DDGS

# ==========================================
# ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š (ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰åˆ¶é™)
# ==========================================
# ã“ã“ã«å¥½ããªãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’è¨­å®šã—ã¦ãã ã•ã„
LOGIN_PASSWORD = "secret_password" 

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ã‚’æ±‚ã‚ã‚‹
with st.sidebar:
    st.markdown("### ğŸ” èªè¨¼")
    input_password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›", type="password")

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒä¸€è‡´ã—ãªã„å ´åˆã€ã“ã“ã§å‡¦ç†ã‚’æ­¢ã‚ã‚‹
if input_password != LOGIN_PASSWORD:
    st.warning("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()  # ã“ã“ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒå¼·åˆ¶åœæ­¢ã—ã¾ã™
    
# ---------------------------------------------------------
# è¨­å®š: è§£æç”¨ã«ã¯SSLè­¦å‘Šã‚’ç„¡è¦–ã™ã‚‹
# ---------------------------------------------------------
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# æ¤œç´¢æ©Ÿèƒ½
# ==========================================
def get_search_results(keyword, max_results=10):
    results = []
    try:
        with DDGS() as ddgs:
            ddg_gen = ddgs.text(keyword, region='jp-jp', timelimit='y', max_results=max_results, backend='html')
            for r in ddg_gen: results.append(r)
    except: return None
    return results

# ==========================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ)
# ==========================================
def generate_local_schema(name, url, phone="03-xxxx-xxxx"):
    data = {
        "@context": "https://schema.org",
        "@type": "SportsActivityLocation",
        "name": name,
        "url": url,
        "telephone": phone,
        "address": {"@type": "PostalAddress", "addressLocality": "å¸‚åŒºç”ºæ‘", "addressRegion": "éƒ½é“åºœçœŒ", "streetAddress": "ç•ªåœ°"},
        "priceRange": "Â¥5,000ã€œÂ¥10,000"
    }
    return json.dumps(data, indent=2, ensure_ascii=False)

def generate_faq_schema():
    data = {
        "@context": "https://schema.org",
        "@type": "FAQPage",
        "mainEntity": [{"@type": "Question", "name": "åˆå¿ƒè€…ã§ã‚‚å¤§ä¸ˆå¤«ã§ã™ã‹ï¼Ÿ", "acceptedAnswer": {"@type": "Answer", "text": "ã¯ã„ã€åˆå¿ƒè€…è¬›ç¿’ã‚’ã”ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚"}}]
    }
    return json.dumps(data, indent=2, ensure_ascii=False)

def generate_table_html():
    return """<table><thead><tr><th>ã‚³ãƒ¼ã‚¹å</th><th>æ–™é‡‘(ç¨è¾¼)</th></tr></thead><tbody><tr><td>ãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼</td><td>10,000å††</td></tr></tbody></table>"""

# ==========================================
# è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def get_page_content(url):
    """ è§£æç”¨: SSLã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦HTMLã‚’å–å¾— """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Mobile Safari/537.36'}
        # verify=False ã§ç„¡ç†ã‚„ã‚Šèª­ã¿è¾¼ã‚€
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup, "OK"
    except Exception as e:
        return None, str(e)

def analyze_keywords(soup, target_keywords_str):
    tasks = []
    if not target_keywords_str: return tasks
    keywords = [k.strip() for k in target_keywords_str.replace("ã€€", " ").split(" ") if k.strip()]
    if not keywords: return tasks

    unit_points = 20 / (len(keywords) * 3)
    title = soup.title.string if soup.title else ""

    h1_tags = soup.find_all('h1')
    h1_text_list = []
    for tag in h1_tags:
        h1_text_list.append(tag.get_text().replace("\n", ""))
        imgs = tag.find_all('img')
        for img in imgs: h1_text_list.append(img.get('alt', ''))
    h1_text = " ".join(h1_text_list)
    body = soup.get_text().replace("\n", "")

    for kw in keywords:
        if kw not in title: tasks.append({"msg": f"Titleä¸å‚™: ã€Œ{kw}ã€ã‚’è¿½åŠ ", "points": unit_points, "tag": "keyword"})
        if kw not in h1_text: tasks.append({"msg": f"H1ä¸å‚™: ã€Œ{kw}ã€ã‚’è¿½åŠ ", "points": unit_points, "tag": "keyword"})
        if kw not in body: tasks.append({"msg": f"æœ¬æ–‡ä¸å‚™: ã€Œ{kw}ã€ã‚’è¿½åŠ ", "points": unit_points, "tag": "keyword"})
    return tasks

def check_local_elements(soup):
    tasks = []
    clean_text = re.sub(r'\s+', '', soup.get_text())
    has_address = any(x in clean_text for x in ["ä½æ‰€", "æ‰€åœ¨åœ°", "ã€’", "éƒ½", "çœŒ", "å¸‚", "åŒº"])
    tel_links = soup.find_all('a', href=re.compile(r'^tel:'))
    has_phone = len(tel_links) > 0 or re.search(r'\d{2,4}-\d{2,4}-\d{4}', soup.get_text())

    has_map = False
    iframes = soup.find_all('iframe')
    map_patterns = ["maps.google", "goo.gl/maps", "googleusercontent.com/maps"]
    for iframe in iframes:
        src = (iframe.get('src') or "") + (iframe.get('data-src') or "") + (iframe.get('data-lazy-src') or "")
        if any(p in src for p in map_patterns): has_map = True; break
    if not has_map:
        scripts = soup.find_all('script')
        for script in scripts:
            src = script.get('src') or ""
            if "maps.googleapis.com" in src: has_map = True; break

    js_map_trace = False
    if not has_map:
        if len(soup.find_all('div', id=re.compile(r'map|Map'), class_=re.compile(r'map|Map'))) > 0: js_map_trace = True

    if not has_address: tasks.append({"msg": "ä½æ‰€ãƒ†ã‚­ã‚¹ãƒˆã®è¿½åŠ ", "points": 7, "tag": "nap"})
    if not has_phone: tasks.append({"msg": "é›»è©±ç•ªå·ãƒªãƒ³ã‚¯(tel:)ã®è¨­å®š", "points": 7, "tag": "nap"})
    if not has_map:
        if js_map_trace: tasks.append({"msg": "Googleãƒãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ (ç¾åœ¨JSè¡¨ç¤ºã®å¯èƒ½æ€§ã‚ã‚Š)", "points": 3, "tag": "nap"})
        else: tasks.append({"msg": "Googleãƒãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ (iframe)", "points": 6, "tag": "nap"})
    return tasks

def check_qa_and_structure(soup):
    tasks = []
    text = soup.get_text()
    faq_kws = ["ã‚ˆãã‚ã‚‹è³ªå•", "ã‚ˆãã‚ã‚‹ã”è³ªå•", "Q&A", "FAQ", "è³ªå•ã¨å›ç­”", "ã”è³ªå•"]
    has_faq = any(k in text for k in faq_kws)
    has_structure = len(soup.find_all(['table', 'dl', 'details'])) > 0

    if not has_faq: tasks.append({"msg": "FAQã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¿½åŠ ", "points": 10, "tag": "structure"})
    if not has_structure: tasks.append({"msg": "æ§‹é€ åŒ–ã‚¿ã‚°(Table/dl)ã§ã®ã‚¹ãƒšãƒƒã‚¯è¡¨è¨˜", "points": 10, "tag": "table_code"})
    return tasks

def check_trust_signals(soup, url):
    """ â‘£ ä¿¡é ¼æ€§ãƒ»E-E-A-T (é…ç‚¹20) - SSLå³æ ¼ãƒã‚§ãƒƒã‚¯è¿½åŠ ç‰ˆ """
    tasks = []
    text = soup.get_text()

    # 1. é‹å–¶è€…æƒ…å ±
    auth_keywords = ["ç›£ä¿®", "è²¬ä»»è€…", "ä»£è¡¨", "é‹å–¶", "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«", "ä¼šç¤¾æ¦‚è¦", "ä¼æ¥­æƒ…å ±", "Company", "About"]
    has_auth = any(k in text for k in auth_keywords)

    # 2. ãƒãƒªã‚·ãƒ¼ãƒªãƒ³ã‚¯
    a_tags = soup.find_all('a', href=True)
    policy_keywords = ["privacy", "policy", "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼", "å€‹äººæƒ…å ±", "ä¿è­·æ–¹é‡"]
    has_policy = False
    for a in a_tags:
        link_url = a.get('href', '').lower()
        link_text = a.get_text().lower()
        if any(kw in link_url for kw in policy_keywords) or any(kw in link_text for kw in policy_keywords):
            has_policy = True; break
    if not has_policy: has_policy = "å€‹äººæƒ…å ±ä¿è­·æ–¹é‡" in text or "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼" in text

    # 3. SSLãƒã‚§ãƒƒã‚¯ (äºŒæ®µéšåˆ¤å®š)
    is_https_scheme = urlparse(url).scheme == "https"
    ssl_valid = False

    if not is_https_scheme:
        # ãã‚‚ãã‚‚ http:// ã§ã‚ã‚‹
        tasks.append({"msg": "å¸¸æ™‚SSLåŒ–(https)å¯¾å¿œ", "points": 10, "tag": "trust"})
    else:
        # https:// ã ãŒã€è¨¼æ˜æ›¸ãŒæ­£ã—ã„ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹ (verify=True)
        try:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ã‚ã§å³æ ¼ãƒã‚§ãƒƒã‚¯
            requests.get(url, timeout=5, verify=True)
            ssl_valid = True
        except requests.exceptions.SSLError:
            # SSLã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ (è¨¼æ˜æ›¸ä¸å‚™ã€æœŸé™åˆ‡ã‚Œã€ã‚ªãƒ¬ã‚ªãƒ¬è¨¼æ˜æ›¸ãªã©)
            tasks.append({"msg": "SSLè¨¼æ˜æ›¸ã®ä¸å‚™ä¿®æ­£ (éµãƒãƒ¼ã‚¯ãŒç„¡åŠ¹ã§ã™)", "points": 10, "tag": "trust"})
        except:
            # ãã®ä»–ã®æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯ä¸€æ—¦ã‚¹ãƒ«ãƒ¼(è§£æç”¨ã§ã¯èª­ã‚ã¦ã„ã‚‹ã®ã§)
            ssl_valid = True

    if not has_auth: tasks.append({"msg": "é‹å–¶è€…æƒ…å ±ãƒªãƒ³ã‚¯ã®è¨­ç½®", "points": 10, "tag": "trust"})

    # ãƒãƒªã‚·ãƒ¼ã¯HTTPSã‹ã¤ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã®ã¿å®Œå…¨OK
    if is_https_scheme and ssl_valid and not has_policy:
         tasks.append({"msg": "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã¸ã®ãƒªãƒ³ã‚¯è¨­ç½®", "points": 10, "tag": "trust"})

    return tasks

def check_tech_schema(soup, base_url):
    tasks = []
    # llms.txtã¯å­˜åœ¨ã™ã‚Œã°OK (SSLç„¡è¦–)
    try:
        if requests.get(urljoin(base_url, "/llms.txt"), timeout=3, verify=False).status_code != 200:
             tasks.append({"msg": "llms.txtã®è¨­ç½®", "points": 5, "tag": "tech"})
    except: tasks.append({"msg": "llms.txtã®è¨­ç½®", "points": 5, "tag": "tech"})

    scripts = soup.find_all('script', type='application/ld+json')
    found_types = []
    for script in scripts:
        try:
            data = json.loads(script.string)
            if isinstance(data, dict):
                 if "@type" in data: found_types.append(data["@type"])
            elif isinstance(data, list):
                for item in data:
                    if "@type" in item: found_types.append(item["@type"])
        except: continue

    if "FAQPage" not in found_types: tasks.append({"msg": "FAQPageæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®è¨˜è¿°", "points": 5, "tag": "faq_code"})
    local_types = ["LocalBusiness", "SportsActivityLocation", "ExerciseGym", "Store", "Restaurant"]
    if not any(t in found_types for t in local_types): tasks.append({"msg": "LocalBusinessæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®è¨˜è¿°", "points": 10, "tag": "local_code"})
    return tasks

# ==========================================
# UIæ§‹ç¯‰
# ==========================================
st.set_page_config(page_title="åº—èˆ—AIOæ”¹å–„ææ¡ˆ", layout="wide")
st.title("ğŸ›¡ï¸ AIO/LLMO è¨ºæ–­ãƒã‚§ãƒƒã‚«ãƒ¼ v21")

st.info("""
**ã€ã‚¹ã‚³ã‚¢ã¨æ¤œç´¢é †ä½ã«é–¢ã™ã‚‹æ³¨é‡ˆã€‘**
* æœ¬ã‚¹ã‚³ã‚¢ã¯ã€AIã®é¸å®šåŸºæº–ï¼ˆå†…éƒ¨è¦å› ï¼‰ã‚’æœ€ä½é™æº€ãŸã—ã¦ã„ã‚‹ã‹ã®æŒ‡æ¨™ã§ã™ã€‚
* **ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã«ã‚‚é–¢ã‚ã‚‰ãšAIã«é¸ã°ã‚Œãªã„ï¼ˆå„ªå…ˆåº¦ãŒä½ã„ï¼‰å ´åˆ**ã¯ã€ä»¥ä¸‹ã®ã€Œå¤–éƒ¨è¦å› ã€ã®å½±éŸ¿åº¦ãŒé«˜ããªã‚Šã¾ã™ã€‚
    1.  **ç¬¬ä¸‰è€…ãƒ¡ãƒ‡ã‚£ã‚¢æ²è¼‰:** æ¯”è¼ƒã‚µã‚¤ãƒˆã‚„ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨˜äº‹ç­‰ã«ä¸Šä½æ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã‹
    2.  **ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‘ãƒ¯ãƒ¼:** æŒ‡åæ¤œç´¢æ•°ã‚„è¢«ãƒªãƒ³ã‚¯æ•°ãŒå¼·ãã€ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›ãŒã‚ã‚‹ã‹
    3.  **MEOè©•ä¾¡:** Googleãƒãƒƒãƒ—ã§ã®å£ã‚³ãƒŸæ•°ã‚„è©•ä¾¡ãŒé«˜ã„ã‹
""")

if 'tasks' not in st.session_state: st.session_state.tasks = []
if 'analyzed' not in st.session_state: st.session_state.analyzed = False
if 'meta_data' not in st.session_state: st.session_state.meta_data = {}
if 'search_results' not in st.session_state: st.session_state.search_results = None
if 'search_error' not in st.session_state: st.session_state.search_error = False

with st.container():
    col1, col2 = st.columns(2)
    with col1:
        company_name = st.text_input("åº—èˆ—å", placeholder="åº—èˆ—åã‚’å…¥åŠ›")
        target_url = st.text_input("åº—èˆ—URL", placeholder="https://...")
    with col2:
        keywords_input = st.text_input("ç‹™ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", placeholder="ã‚¨ãƒªã‚¢ æ¥­ç¨® ãŠã™ã™ã‚")

    analyze_btn = st.button("è¨ºæ–­ã‚¹ã‚¿ãƒ¼ãƒˆ", type="primary")

# è§£æãƒ­ã‚¸ãƒƒã‚¯
if analyze_btn and target_url:
    with st.spinner("è§£æãƒ»ç«¶åˆèª¿æŸ»ãƒ»SSLå¼·åº¦åˆ¤å®šä¸­..."):
        # 1. è‡ªç¤¾ã‚µã‚¤ãƒˆè§£æ (verify=Falseã§ä¸­èº«ã‚’å–å¾—)
        soup, status = get_page_content(target_url)
        if soup:
            t1 = analyze_keywords(soup, keywords_input)
            t2 = check_local_elements(soup)
            t3 = check_qa_and_structure(soup)
            t4 = check_trust_signals(soup, target_url) # ã“ã“ã§SSLå³æ ¼ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
            t5 = check_tech_schema(soup, target_url)

            st.session_state.tasks = t1 + t2 + t3 + t4 + t5
            st.session_state.meta_data = {"url": target_url, "name": company_name, "keyword": keywords_input}
            st.session_state.analyzed = True

            # 2. æ¤œç´¢å®Ÿè¡Œ
            if keywords_input:
                results = get_search_results(keywords_input)
                if results is None:
                    st.session_state.search_results = []
                    st.session_state.search_error = True
                else:
                    st.session_state.search_results = results
                    st.session_state.search_error = False
        else:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {status}")

# çµæœç”»é¢
if st.session_state.analyzed:

    # æ¤œç´¢çµæœï¼ˆç«¶åˆä¸€è¦§ï¼‰ã‚¨ãƒªã‚¢
    st.divider()
    st.subheader("ğŸ“Š æ¤œç´¢ä¸Šä½ã‚µã‚¤ãƒˆ (AIã®å‚ç…§å…ƒå€™è£œ)")

    if st.session_state.search_error:
        st.error("âš ï¸ æ¤œç´¢çµæœã®è‡ªå‹•å–å¾—ãŒåˆ¶é™ã•ã‚Œã¾ã—ãŸã€‚")
        st.markdown(f"ä»¥ä¸‹ã®ãƒœã‚¿ãƒ³ã§ã€å®Ÿéš›ã®Googleæ¤œç´¢çµæœã‚’ç›´æ¥ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        google_url = f"https://www.google.com/search?q={quote(st.session_state.meta_data['keyword'])}"
        st.link_button("Googleæ¤œç´¢çµæœã‚’åˆ¥ã‚¿ãƒ–ã§é–‹ã", google_url)

    elif st.session_state.search_results:
        st.markdown("ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆã«ã€Œå¾¡ç¤¾ã®åå‰ã€ãŒæ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        with st.expander("ä¸Šä½10ã‚µã‚¤ãƒˆã‚’è¡¨ç¤º", expanded=True):
            for i, res in enumerate(st.session_state.search_results, 1):
                icon = "ğŸ”—"
                if any(k in res['title'] for k in ["ãŠã™ã™ã‚", "é¸", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "æ¯”è¼ƒ"]):
                    icon = "ğŸ‘‘"
                    st.markdown(f"**{i}. {icon} [{res['title']}]({res['href']})**")
                else:
                    st.markdown(f"{i}. {icon} [{res['title']}]({res['href']})")
    else:
        st.warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # ã‚¹ã‚³ã‚¢è¨ˆç®—
    current_deduction = 0
    st.divider()
    c1, c2 = st.columns([1, 2])

    with c2:
        st.subheader("ğŸ“ æ”¹å–„ã‚¿ã‚¹ã‚¯ (Check to Resolve)")
        if not st.session_state.tasks:
            st.success("å®Œç’§ã§ã™ã€‚")
        else:
            st.markdown("ç›®è¦–ã§ç¢ºèªã§ããŸã‚‚ã®ã€ã¾ãŸã¯å¯¾å¿œã—ãŸé …ç›®ã«**ãƒã‚§ãƒƒã‚¯**ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
            for i, task in enumerate(st.session_state.tasks):
                pt_display = round(task['points'], 1)
                label = f"**{task['msg']}** (é…ç‚¹: {pt_display}ç‚¹)"
                checked = st.checkbox(label, key=f"task_{i}")
                if not checked:
                    current_deduction += task['points']

    final_score = max(0, int(100 - current_deduction))

    with c1:
        st.metric("ç¾åœ¨ã®AIOé©åˆã‚¹ã‚³ã‚¢", f"{final_score} / 100")
        st.progress(final_score / 100)

        if final_score >= 80: st.info("ã‚µã‚¤ãƒˆå†…éƒ¨ã¯åˆæ ¼åœå†…ã§ã™ã€‚")
        else: st.warning("æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚")

    # ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
    st.divider()
    st.subheader("ğŸ’¡ å¿…è¦ãªæ”¹å–„ã‚³ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«")
    has_code = False
    active_tags = [st.session_state.tasks[i]['tag'] for i in range(len(st.session_state.tasks)) if not st.session_state.get(f"task_{i}", False)]

    if "local_code" in active_tags:
        st.markdown("#### 1. åº—èˆ—æƒ…å ±ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ (LocalBusiness)")
        st.code(generate_local_schema(st.session_state.meta_data['name'], st.session_state.meta_data['url']), language='json')
        has_code = True

    if "faq_code" in active_tags:
        st.markdown("#### 2. FAQã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ (FAQPage)")
        st.code(generate_faq_schema(), language='json')
        has_code = True

    if "table_code" in active_tags:
        st.markdown("#### 3. æ–™é‡‘è¡¨ãªã©ã®HTMLè¨˜è¿°ä¾‹")
        st.code(generate_table_html(), language='html')
        has_code = True

    if "nap" in active_tags:
        st.markdown("#### 4. é›»è©±ç•ªå·ãƒªãƒ³ã‚¯è¨˜è¿°ä¾‹")
        st.code('<a href="tel:03-xxxx-xxxx">03-xxxx-xxxx</a>', language='html')
        has_code = True

    if not has_code:
        st.caption("ç¾åœ¨è¡¨ç¤ºã™ã¹ãã‚³ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

  # 1. æœ€æ–°ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install -U -q duckduckgo_search streamlit

# 2. Cloudflare Tunnel ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64
!chmod +x cloudflared-linux-amd64

# 3. Streamlitã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•
import subprocess
subprocess.Popen(["streamlit", "run", "app.py"])

# 4. ãƒˆãƒ³ãƒãƒ«ã‚’ä½œæˆã—ã¦URLã‚’è¡¨ç¤º
import time
print("ğŸš€ ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¦ã„ã¾ã™... ã—ã°ã‚‰ããŠå¾…ã¡ä¸‹ã•ã„ (ç´„10ç§’)")
time.sleep(5)

with open('cloudflared.log', 'w') as f:
    subprocess.Popen(['./cloudflared-linux-amd64', 'tunnel', '--url', 'http://localhost:8501'], stdout=f, stderr=f)

time.sleep(8)
print("\nğŸ‘‡ ä»¥ä¸‹ã®URLã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è¨ºæ–­ãƒ„ãƒ¼ãƒ«ã‚’é–‹ã„ã¦ãã ã•ã„ï¼ˆãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä¸è¦ï¼‰")
!grep -o 'https://.*\.trycloudflare.com' cloudflared.log | head -n 1
